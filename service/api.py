from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional, List
import uvicorn
import os
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥–∞–º
current_dir = os.path.dirname(os.path.abspath(__file__))
configs_dir = os.path.join(current_dir, "..", "configs")
sys.path.insert(0, configs_dir)

# –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
try:
    from config_loader import config
    HAS_CONFIG = True
except ImportError:
    HAS_CONFIG = False
    print("‚ö†Ô∏è config_loader –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä
from predictor import ModelPredictor

# –°–æ–∑–¥–∞–µ–º FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = FastAPI(
    title="NLP MLOps API",
    description="API –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –ø–æ —Ç–µ–∫—Å—Ç—É –ø–æ—Å—Ç–∞",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# –ü—É—Ç–∏ –∫ –º–æ–¥–µ–ª—è–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
DEFAULT_MODEL_PATH = os.path.join(current_dir, "models", "best_model.pkl")
DEFAULT_VECTORIZER_PATH = os.path.join(current_dir, "models", "tfidf_vectorizer.pkl")

# –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
if HAS_CONFIG:
    try:
        inference_config = config.get_inference_config()
        model_path = inference_config.model.model_path
        vectorizer_path = inference_config.model.vectorizer_path
        print(f"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞:")
        print(f"   –ú–æ–¥–µ–ª—å: {model_path}")
        print(f"   –í–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä: {vectorizer_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥–∞: {e}")
        model_path = DEFAULT_MODEL_PATH
        vectorizer_path = DEFAULT_VECTORIZER_PATH
else:
    model_path = DEFAULT_MODEL_PATH
    vectorizer_path = DEFAULT_VECTORIZER_PATH
    print(f"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é:")
    print(f"   –ú–æ–¥–µ–ª—å: {model_path}")
    print(f"   –í–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä: {vectorizer_path}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
print(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–µ–π:")
print(f"   –ú–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(model_path)}")
print(f"   –í–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(vectorizer_path)}")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä
predictor = ModelPredictor(
    model_path=model_path,
    vectorizer_path=vectorizer_path
)

# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö (Pydantic —Å—Ö–µ–º—ã)
class PredictRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
    text: str
    model_type: Optional[str] = "ridge"

class PredictResponse(BaseModel):
    """–û—Ç–≤–µ—Ç —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º"""
    prediction: float
    processing_time_ms: float
    features_count: Optional[int] = None
    error: Optional[str] = None

class BatchPredictRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –¥–ª—è batch –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
    texts: List[str]

class HealthResponse(BaseModel):
    """–û—Ç–≤–µ—Ç –¥–ª—è health check"""
    status: str
    model_loaded: bool
    model_type: Optional[str] = None

# –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã (–æ—Å—Ç–∞–≤–ª—è–µ–º –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
@app.get("/", tags=["Root"])
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç"""
    return {
        "service": "Comment Prediction API",
        "version": "1.0.0",
        "endpoints": {
            "documentation": "/docs",
            "health_check": "/health",
            "single_prediction": "/predict",
            "batch_prediction": "/predict/batch"
        }
    }

@app.get("/health", response_model=HealthResponse, tags=["Health"])
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    info = predictor.get_model_info()
    return HealthResponse(
        status="healthy" if info["is_loaded"] else "degraded",
        model_loaded=info["is_loaded"],
        model_type=info.get("model_type")
    )

@app.post("/predict", response_model=PredictResponse, tags=["Prediction"])
async def predict_single(request: PredictRequest):
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    if not request.text.strip():
        raise HTTPException(status_code=400, detail="Text cannot be empty")
    
    result = predictor.predict(request.text)
    
    if result["error"]:
        raise HTTPException(status_code=500, detail=result["error"])
    
    return PredictResponse(**result)

@app.post("/predict/batch", tags=["Prediction"])
async def predict_batch(request: BatchPredictRequest):
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
    if not request.texts:
        raise HTTPException(status_code=400, detail="Texts list cannot be empty")
    
    results = predictor.batch_predict(request.texts)
    return {"predictions": results}

@app.get("/model/info", tags=["Model"])
async def model_info():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    return predictor.get_model_info()

# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
if __name__ == "__main__":
    uvicorn.run(
        "api:app",
        host="0.0.0.0",
        port=8000,
        reload=False
    )