import gradio as gr
import requests
import time
import json

class MLServiceClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å ML —Å–µ—Ä–≤–∏—Å–∞–º–∏"""
    
    def __init__(self):
        self.fastapi_url = "http://localhost:8000"
        self.bentoml_url = "http://localhost:3000"
    
    def predict_fastapi_single(self, text):
        """Single prediction —á–µ—Ä–µ–∑ FastAPI"""
        try:
            start = time.time()
            response = requests.post(
                f"{self.fastapi_url}/predict",
                json={"text": text},
                timeout=5
            )
            latency = time.time() - start
            
            if response.status_code == 200:
                result = response.json()
                result["service"] = "FastAPI"
                result["type"] = "single"
                result["latency_ms"] = round(latency * 1000, 1)
                return result
            return {"error": f"HTTP {response.status_code}", "service": "FastAPI"}
        except Exception as e:
            return {"error": str(e), "service": "FastAPI"}
    
    def predict_bentoml_single(self, text):
        
        try:
            start = time.time()
            
            # –°–ù–ê–ß–ê–õ–ê –ø—Ä–æ–±—É–µ–º text/plain (–æ—Å–Ω–æ–≤–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç)
            response = requests.post(
                "http://localhost:3000/predict",
                data=text,  # data=, –Ω–µ json=
                headers={"Content-Type": "text/plain"},
                timeout=5
            )
            
            # –ï—Å–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ, –ø—Ä–æ–±—É–µ–º JSON
            if response.status_code != 200:
                response = requests.post(
                    "http://localhost:3000/predict",
                    json={"text": text},  # json=
                    timeout=5
                )
            
            latency = time.time() - start
            
            if response.status_code == 200:
                result = response.json()
                result["latency_ms"] = round(latency * 1000, 1)
                result["service"] = "BentoML"
                return result
            else:
                return {
                    "error": f"HTTP {response.status_code}: {response.text[:100]}",
                    "service": "BentoML"
                }
                
        except Exception as e:
            return {"error": str(e), "service": "BentoML"}
        
    def predict_fastapi_batch(self, texts):
        """Batch prediction —á–µ—Ä–µ–∑ FastAPI"""
        try:
            start = time.time()
            response = requests.post(
                f"{self.fastapi_url}/predict/batch",
                json={"texts": texts},
                timeout=10
            )
            latency = time.time() - start
            
            if response.status_code == 200:
                result = response.json()
                result["service"] = "FastAPI"
                result["type"] = "batch"
                result["latency_ms"] = round(latency * 1000, 1)
                result["texts_count"] = len(texts)
                return result
            return {"error": f"HTTP {response.status_code}", "service": "FastAPI"}
        except Exception as e:
            return {"error": str(e), "service": "FastAPI"}
    
    def predict_bentoml_batch(self, texts):
        """Batch prediction —á–µ—Ä–µ–∑ BentoML"""
        try:
            start = time.time()
            response = requests.post(
                f"{self.bentoml_url}/predict_batch",
                json={"texts": texts},
                timeout=10
            )
            latency = time.time() - start
            
            if response.status_code == 200:
                result = response.json()
                result["service"] = "BentoML"
                result["type"] = "batch"
                result["latency_ms"] = round(latency * 1000, 1)
                result["texts_count"] = len(texts)
                return result
            return {"error": f"HTTP {response.status_code}", "service": "BentoML"}
        except Exception as e:
            return {"error": str(e), "service": "BentoML"}

# –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç
client = MLServiceClient()

def test_single_prediction(text):
    """–¢–µ—Å—Ç single prediction"""
    if not text.strip():
        return {}, {}, {}
    
    fastapi_result = client.predict_fastapi_single(text)
    bentoml_result = client.predict_bentoml_single(text)
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
    comparison = {
        "test_type": "single",
        "fastapi_latency": fastapi_result.get("latency_ms", 0),
        "bentoml_latency": bentoml_result.get("latency_ms", 0),
        "faster_service": ""
    }
    
    fast_lat = fastapi_result.get("latency_ms", float('inf'))
    bento_lat = bentoml_result.get("latency_ms", float('inf'))
    
    if fast_lat < bento_lat:
        comparison["faster_service"] = "FastAPI"
        comparison["difference_ms"] = round(bento_lat - fast_lat, 1)
    else:
        comparison["faster_service"] = "BentoML"
        comparison["difference_ms"] = round(fast_lat - bento_lat, 1)
    
    return fastapi_result, bentoml_result, comparison

def test_batch_prediction(texts_input):
    """–¢–µ—Å—Ç batch prediction"""
    if not texts_input.strip():
        return {}, {}, {}
    
    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å—Ç—Ä–æ–∫–∏
    texts = [t.strip() for t in texts_input.split('\n') if t.strip()]
    
    if len(texts) < 2:
        return {"error": "–ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 2 —Ç–µ–∫—Å—Ç–∞"}, {"error": "–ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 2 —Ç–µ–∫—Å—Ç–∞"}, {}
    
    fastapi_result = client.predict_fastapi_batch(texts)
    bentoml_result = client.predict_bentoml_batch(texts)
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
    comparison = {
        "test_type": "batch",
        "texts_count": len(texts),
        "fastapi_latency": fastapi_result.get("latency_ms", 0),
        "bentoml_latency": bentoml_result.get("latency_ms", 0),
        "avg_time_per_text": {}
    }
    
    fast_lat = fastapi_result.get("latency_ms", 0)
    bento_lat = bentoml_result.get("latency_ms", 0)
    
    if fast_lat > 0 and bento_lat > 0:
        comparison["avg_time_per_text"] = {
            "FastAPI": round(fast_lat / len(texts), 2),
            "BentoML": round(bento_lat / len(texts), 2)
        }
        
        if fast_lat < bento_lat:
            comparison["faster_service"] = "FastAPI"
            comparison["difference_ms"] = round(bento_lat - fast_lat, 1)
            comparison["difference_percent"] = round((bento_lat - fast_lat) / bento_lat * 100, 1)
        else:
            comparison["faster_service"] = "BentoML"
            comparison["difference_ms"] = round(fast_lat - bento_lat, 1)
            comparison["difference_percent"] = round((fast_lat - bento_lat) / fast_lat * 100, 1)
    
    return fastapi_result, bentoml_result, comparison

# –ü—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è batch —Ç–µ—Å—Ç–∞
batch_examples = """–ú–µ—Ç—Ä–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ!
–ü—Ä–æ–±–∫–∏ —Å–µ–≥–æ–¥–Ω—è –Ω–µ–≤—ã–Ω–æ—Å–∏–º—ã–µ
–ù–æ–≤—ã–µ —Å—Ç–∞–Ω—Ü–∏–∏ –æ—á–µ–Ω—å –∫—Ä–∞—Å–∏–≤—ã–µ
–í —á–∞—Å –ø–∏–∫ –Ω–µ –ø—Ä–æ—Ç–æ–ª–∫–Ω—É—Ç—å—Å—è
–≠–ª–µ–∫—Ç—Ä–∏—á–∫–∏ —Ö–æ–¥—è—Ç –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é
–ü–∞—Ä–∫–æ–≤–∫–∞ –≤ —Ü–µ–Ω—Ç—Ä–µ - –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∞
–û–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –ª—É—á—à–µ
–¶–µ–Ω—ã –Ω–∞ –ø—Ä–æ–µ–∑–¥ —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–µ"""

# –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å –≤–∫–ª–∞–¥–∫–∞–º–∏
with gr.Blocks(title="ML Services Comparison") as demo:
    
    gr.Markdown("#–°—Ä–∞–≤–Ω–µ–Ω–∏–µ FastAPI –∏ BentoML")
    gr.Markdown("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ single –∏ batch –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
    
    with gr.Tabs():
        # –í–∫–ª–∞–¥–∫–∞ 1: Single prediction
        with gr.TabItem("Single Prediction"):
            gr.Markdown("## üìù Single Prediction (–æ–¥–∏–Ω —Ç–µ–∫—Å—Ç)")
            
            single_text = gr.Textbox(
                label="–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç",
                placeholder="–ü—Ä–∏–º–µ—Ä: –ú–µ—Ç—Ä–æ —Å–µ–≥–æ–¥–Ω—è —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ!",
                lines=3
            )
            
            single_btn = gr.Button("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å Single", variant="primary")
            
            with gr.Row():
                single_fastapi = gr.JSON(label="FastAPI —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                single_bentoml = gr.JSON(label="BentoML —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
            
            single_comparison = gr.JSON(label="‚öñÔ∏è –°—Ä–∞–≤–Ω–µ–Ω–∏–µ")
            
            single_btn.click(
                fn=test_single_prediction,
                inputs=single_text,
                outputs=[single_fastapi, single_bentoml, single_comparison]
            )
            
            gr.Examples(
                examples=[
                    ["–ú–µ—Ç—Ä–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ, –ø–æ–µ–∑–¥–∞ —Ö–æ–¥—è—Ç –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é!"],
                    ["–£–∂–∞—Å–Ω—ã–µ –ø—Ä–æ–±–∫–∏ –Ω–∞ –∫–æ–ª—å—Ü–µ–≤–æ–π –ª–∏–Ω–∏–∏"],
                    ["–ù–æ–≤—ã–µ –ø–æ–µ–∑–¥–∞ –æ—á–µ–Ω—å –∫–æ–º—Ñ–æ—Ä—Ç–Ω—ã–µ –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ"],
                    ["–í —á–∞—Å –ø–∏–∫ –≤ –º–µ—Ç—Ä–æ –Ω–∞—Å—Ç–æ—è—â–∏–π –∞–¥"]
                ],
                inputs=single_text
            )
        
        # –í–∫–ª–∞–¥–∫–∞ 2: Batch prediction
        with gr.TabItem("Batch Prediction"):
            gr.Markdown("## üìö Batch Prediction (–Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤)")
            gr.Markdown("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç—ã, –∫–∞–∂–¥—ã–π —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏")
            
            batch_texts = gr.Textbox(
                label="–¢–µ–∫—Å—Ç—ã (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É)",
                placeholder="–í–≤–µ–¥–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤...",
                lines=8,
                value=batch_examples
            )
            
            batch_btn = gr.Button("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å Batch", variant="primary")
            
            with gr.Row():
                batch_fastapi = gr.JSON(label="FastAPI batch —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                batch_bentoml = gr.JSON(label="BentoML batch —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
            
            batch_comparison = gr.JSON(label="‚öñÔ∏è –°—Ä–∞–≤–Ω–µ–Ω–∏–µ batch")
            
            batch_btn.click(
                fn=test_batch_prediction,
                inputs=batch_texts,
                outputs=[batch_fastapi, batch_bentoml, batch_comparison]
            )
            
            gr.Markdown("### üìä Batch –º–µ—Ç—Ä–∏–∫–∏:")
            gr.Markdown("""
            - **Total texts**: –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤
            - **Latency**: –æ–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–º—Å)
            - **Avg time per text**: —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç
            - **Throughput**: —Ç–µ–∫—Å—Ç–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
            - **Predictions summary**: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (min, max, mean, std)
            """)
        
        # –í–∫–ª–∞–¥–∫–∞ 3: –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        with gr.TabItem("–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞"):
            gr.Markdown("## ü©∫ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ—Ä–≤–∏—Å–æ–≤")
            
            def check_services():
                          
                results = {}
                
                # FastAPI - –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω–æ
                try:
                    resp = requests.get("http://localhost:8000/health", timeout=3)
                    results["FastAPI"] = {
                        "status": "‚úÖ –î–æ—Å—Ç—É–ø–µ–Ω" if resp.status_code == 200 else "‚ùå –û—à–∏–±–∫–∞",
                        "code": resp.status_code,
                        "has_health": True,
                        "docs": "http://localhost:8000/docs"
                    }
                except:
                    results["FastAPI"] = {
                        "status": "‚ùå –ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω",
                        "has_health": True
                    }
                
                # BentoML - –Ω–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º health, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
                results["BentoML"] = {
                    "status": "üîß –ü—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–∞—Ö",
                    "note": "BentoML –Ω–µ –∏–º–µ–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ /health endpoint",
                    "check_method": "–¢–µ—Å—Ç–æ–≤—ã–π POST –∑–∞–ø—Ä–æ—Å –Ω–∞ /predict",
                    "urls": {
                        "single_predict": "POST http://localhost:3000/predict",
                        "batch_predict": "POST http://localhost:3000/predict_batch"
                    }
                }
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø–æ—Ä—Ç–∞
                try:
                    # –ü—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–æ—Ä—Ç –æ—Ç–∫—Ä—ã—Ç
                    import socket
                    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                    sock.settimeout(2)
                    result = sock.connect_ex(('localhost', 3000))
                    sock.close()
                    
                    if result == 0:
                        results["BentoML"]["port_check"] = "‚úÖ –ü–æ—Ä—Ç 3000 –æ—Ç–∫—Ä—ã—Ç"
                    else:
                        results["BentoML"]["port_check"] = "‚ùå –ü–æ—Ä—Ç 3000 –∑–∞–∫—Ä—ã—Ç"
                except:
                    results["BentoML"]["port_check"] = "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ—Ä—Ç"
                
                return results
            gr.Markdown("### üìã –ü—Ä–∏–º–µ—Ä—ã batch –∑–∞–ø—Ä–æ—Å–æ–≤:")
            gr.Markdown("""
            ```bash
            # FastAPI batch
            curl -X POST http://localhost:8000/predict/batch \\
              -H "Content-Type: application/json" \\
              -d '{"texts": ["–¢–µ–∫—Å—Ç 1", "–¢–µ–∫—Å—Ç 2", "–¢–µ–∫—Å—Ç 3"]}'
            
            # BentoML batch  
            curl -X POST http://localhost:3000/predict_batch \\
              -H "Content-Type: application/json" \\
              -d '{"texts": ["–¢–µ–∫—Å—Ç 1", "–¢–µ–∫—Å—Ç 2", "–¢–µ–∫—Å—Ç 3"]}'
            
            # BentoML single (text/plain)
            curl -X POST http://localhost:3000/predict \\
              -H "Content-Type: text/plain" \\
              -d "–¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"
            ```
            """)

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )