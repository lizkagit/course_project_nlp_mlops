import gradio as gr
import requests
import time
import json

class MLServiceClient:
    def __init__(self):
        self.fastapi_url = "http://localhost:8000"
        self.bentoml_url = "http://localhost:3000"

    def predict_fastapi_single(self, text):
        try:
            start = time.time()
            response = requests.post(f"{self.fastapi_url}/predict", json={"text": text}, timeout=5)
            latency = round((time.time() - start) * 1000, 1)
            return {**response.json(), "service": "FastAPI", "type": "single", "latency_ms": latency} if response.status_code == 200 else {"error": f"HTTP {response.status_code}", "service": "FastAPI"}
        except Exception as e:
            return {"error": str(e), "service": "FastAPI"}

    def predict_bentoml_single(self, text):
        try:
            start = time.time()
            response = requests.post(f"{self.bentoml_url}/predict", data=text, headers={"Content-Type": "text/plain"}, timeout=5)
            if response.status_code != 200:
                response = requests.post(f"{self.bentoml_url}/predict", json={"text": text}, timeout=5)
            latency = round((time.time() - start) * 1000, 1)
            return {**response.json(), "latency_ms": latency, "service": "BentoML"} if response.status_code == 200 else {"error": f"HTTP {response.status_code}: {response.text[:100]}", "service": "BentoML"}
        except Exception as e:
            return {"error": str(e), "service": "BentoML"}

    def predict_fastapi_batch(self, texts):
        try:
            start = time.time()
            response = requests.post(f"{self.fastapi_url}/predict/batch", json={"texts": texts}, timeout=10)
            latency = round((time.time() - start) * 1000, 1)
            return {**response.json(), "service": "FastAPI", "type": "batch", "latency_ms": latency, "texts_count": len(texts)} if response.status_code == 200 else {"error": f"HTTP {response.status_code}", "service": "FastAPI"}
        except Exception as e:
            return {"error": str(e), "service": "FastAPI"}

    def predict_bentoml_batch(self, texts):
        try:
            start = time.time()
            response = requests.post(f"{self.bentoml_url}/predict_batch", json={"texts": texts}, timeout=10)
            latency = round((time.time() - start) * 1000, 1)
            return {**response.json(), "service": "BentoML", "type": "batch", "latency_ms": latency, "texts_count": len(texts)} if response.status_code == 200 else {"error": f"HTTP {response.status_code}", "service": "BentoML"}
        except Exception as e:
            return {"error": str(e), "service": "BentoML"}

client = MLServiceClient()

def test_single_prediction(text):
    if not text.strip():
        return {}, {}, {}

    fastapi_result = client.predict_fastapi_single(text)
    bentoml_result = client.predict_bentoml_single(text)

    fast_lat = fastapi_result.get("latency_ms", float('inf'))
    bento_lat = bentoml_result.get("latency_ms", float('inf'))

    comparison = {
        "test_type": "single",
        "fastapi_latency": fast_lat,
        "bentoml_latency": bento_lat,
        "faster_service": "FastAPI" if fast_lat < bento_lat else "BentoML",
        "difference_ms": round(abs(fast_lat - bento_lat), 1)
    }

    return fastapi_result, bentoml_result, comparison

def test_batch_prediction(texts_input):
    if not texts_input.strip():
        return {}, {}, {}

    texts = [t.strip() for t in texts_input.split('\n') if t.strip()]
    if len(texts) < 2:
        return {"error": "ÐÑƒÐ¶Ð½Ð¾ Ð¼Ð¸Ð½Ð¸Ð¼ÑƒÐ¼ 2 Ñ‚ÐµÐºÑÑ‚Ð°"}, {"error": "ÐÑƒÐ¶Ð½Ð¾ Ð¼Ð¸Ð½Ð¸Ð¼ÑƒÐ¼ 2 Ñ‚ÐµÐºÑÑ‚Ð°"}, {}

    fastapi_result = client.predict_fastapi_batch(texts)
    bentoml_result = client.predict_bentoml_batch(texts)

    fast_lat = fastapi_result.get("latency_ms", 0)
    bento_lat = bentoml_result.get("latency_ms", 0)

    comparison = {
        "test_type": "batch",
        "texts_count": len(texts),
        "fastapi_latency": fast_lat,
        "bentoml_latency": bento_lat,
        "avg_time_per_text": {
            "FastAPI": round(fast_lat / len(texts), 2) if fast_lat > 0 else 0,
            "BentoML": round(bento_lat / len(texts), 2) if bento_lat > 0 else 0
        }
    }

    if fast_lat > 0 and bento_lat > 0:
        if fast_lat < bento_lat:
            comparison.update({
                "faster_service": "FastAPI",
                "difference_ms": round(bento_lat - fast_lat, 1),
                "difference_percent": round((bento_lat - fast_lat) / bento_lat * 100, 1)
            })
        else:
            comparison.update({
                "faster_service": "BentoML",
                "difference_ms": round(fast_lat - bento_lat, 1),
                "difference_percent": round((fast_lat - bento_lat) / fast_lat * 100, 1)
            })

    return fastapi_result, bentoml_result, comparison

batch_examples = """ÐœÐµÑ‚Ñ€Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ð¾!
ÐŸÑ€Ð¾Ð±ÐºÐ¸ ÑÐµÐ³Ð¾Ð´Ð½Ñ Ð½ÐµÐ²Ñ‹Ð½Ð¾ÑÐ¸Ð¼Ñ‹Ðµ
ÐÐ¾Ð²Ñ‹Ðµ ÑÑ‚Ð°Ð½Ñ†Ð¸Ð¸ Ð¾Ñ‡ÐµÐ½ÑŒ ÐºÑ€Ð°ÑÐ¸Ð²Ñ‹Ðµ
Ð’ Ñ‡Ð°Ñ Ð¿Ð¸Ðº Ð½Ðµ Ð¿Ñ€Ð¾Ñ‚Ð¾Ð»ÐºÐ½ÑƒÑ‚ÑŒÑÑ
Ð­Ð»ÐµÐºÑ‚Ñ€Ð¸Ñ‡ÐºÐ¸ Ñ…Ð¾Ð´ÑÑ‚ Ð¿Ð¾ Ñ€Ð°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸ÑŽ
ÐŸÐ°Ñ€ÐºÐ¾Ð²ÐºÐ° Ð² Ñ†ÐµÐ½Ñ‚Ñ€Ðµ - ÐºÐ°Ñ‚Ð°ÑÑ‚Ñ€Ð¾Ñ„Ð°
ÐžÐ±Ñ‰ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ñ‚Ñ€Ð°Ð½ÑÐ¿Ð¾Ñ€Ñ‚ ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑÑ Ð»ÑƒÑ‡ÑˆÐµ
Ð¦ÐµÐ½Ñ‹ Ð½Ð° Ð¿Ñ€Ð¾ÐµÐ·Ð´ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð²Ñ‹ÑÐ¾ÐºÐ¸Ðµ"""

with gr.Blocks(title="ML Services Comparison") as demo:
    gr.Markdown("# Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ FastAPI Ð¸ BentoML")
    gr.Markdown("Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ single Ð¸ batch Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¹")

    with gr.Tabs():
        with gr.TabItem("Single Prediction"):
            gr.Markdown("## ðŸ“ Single Prediction (Ð¾Ð´Ð¸Ð½ Ñ‚ÐµÐºÑÑ‚)")
            single_text = gr.Textbox(label="Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ñ‚ÐµÐºÑÑ‚", placeholder="ÐŸÑ€Ð¸Ð¼ÐµÑ€: ÐœÐµÑ‚Ñ€Ð¾ ÑÐµÐ³Ð¾Ð´Ð½Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ð¾!", lines=3)
            single_btn = gr.Button("ðŸš€ Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Single", variant="primary")

            with gr.Row():
                single_fastapi = gr.JSON(label="FastAPI Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚")
                single_bentoml = gr.JSON(label="BentoML Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚")

            single_comparison = gr.JSON(label="âš–ï¸ Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ")

            single_btn.click(test_single_prediction, inputs=single_text, outputs=[single_fastapi, single_bentoml, single_comparison])

            gr.Examples(
                examples=[
                    ["ÐœÐµÑ‚Ñ€Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ð¾, Ð¿Ð¾ÐµÐ·Ð´Ð° Ñ…Ð¾Ð´ÑÑ‚ Ð¿Ð¾ Ñ€Ð°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸ÑŽ!"],
                    ["Ð£Ð¶Ð°ÑÐ½Ñ‹Ðµ Ð¿Ñ€Ð¾Ð±ÐºÐ¸ Ð½Ð° ÐºÐ¾Ð»ÑŒÑ†ÐµÐ²Ð¾Ð¹ Ð»Ð¸Ð½Ð¸Ð¸"],
                    ["ÐÐ¾Ð²Ñ‹Ðµ Ð¿Ð¾ÐµÐ·Ð´Ð° Ð¾Ñ‡ÐµÐ½ÑŒ ÐºÐ¾Ð¼Ñ„Ð¾Ñ€Ñ‚Ð½Ñ‹Ðµ Ð¸ ÑÐ¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ"],
                    ["Ð’ Ñ‡Ð°Ñ Ð¿Ð¸Ðº Ð² Ð¼ÐµÑ‚Ñ€Ð¾ Ð½Ð°ÑÑ‚Ð¾ÑÑ‰Ð¸Ð¹ Ð°Ð´"]
                ],
                inputs=single_text
            )

        with gr.TabItem("Batch Prediction"):
            gr.Markdown("## ðŸ“š Batch Prediction (Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²)")
            gr.Markdown("Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ñ‚ÐµÐºÑÑ‚Ñ‹, ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ñ Ð½Ð¾Ð²Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¸")
            batch_texts = gr.Textbox(label="Ð¢ÐµÐºÑÑ‚Ñ‹ (Ð¿Ð¾ Ð¾Ð´Ð½Ð¾Ð¼Ñƒ Ð½Ð° ÑÑ‚Ñ€Ð¾ÐºÑƒ)", placeholder="Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²...", lines=8, value=batch_examples)
            batch_btn = gr.Button("ðŸš€ Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Batch", variant="primary")

            with gr.Row():
                batch_fastapi = gr.JSON(label="FastAPI batch Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚")
                batch_bentoml = gr.JSON(label="BentoML batch Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚")

            batch_comparison = gr.JSON(label="âš–ï¸ Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ batch")

            batch_btn.click(test_batch_prediction, inputs=batch_texts, outputs=[batch_fastapi, batch_bentoml, batch_comparison])

            gr.Markdown("### ðŸ“Š Batch Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸:")
            gr.Markdown("""
            - **Total texts**: Ð¾Ð±Ñ‰ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²
            - **Latency**: Ð¾Ð±Ñ‰ÐµÐµ Ð²Ñ€ÐµÐ¼Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ (Ð¼Ñ)
            - **Avg time per text**: ÑÑ€ÐµÐ´Ð½ÐµÐµ Ð²Ñ€ÐµÐ¼Ñ Ð½Ð° Ð¾Ð´Ð¸Ð½ Ñ‚ÐµÐºÑÑ‚
            - **Throughput**: Ñ‚ÐµÐºÑÑ‚Ð¾Ð² Ð² ÑÐµÐºÑƒÐ½Ð´Ñƒ
            - **Predictions summary**: ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¹ (min, max, mean, std)
            """)

        with gr.TabItem("Ð”Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ°"):
            gr.Markdown("## ðŸ©º ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐµÑ€Ð²Ð¸ÑÐ¾Ð²")

            def check_services():
                results = {"FastAPI": {"status": "âŒ ÐÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½"}, "BentoML": {"status": "ðŸ”§ ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ÑÑ Ð¿Ñ€Ð¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°Ñ…", "urls": {"single_predict": "POST http://localhost:3000/predict", "batch_predict": "POST http://localhost:3000/predict_batch"}}}

                try:
                    resp = requests.get("http://localhost:8000/health", timeout=3)
                    results["FastAPI"] = {"status": "âœ… Ð”Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½" if resp.status_code == 200 else "âŒ ÐžÑˆÐ¸Ð±ÐºÐ°", "code": resp.status_code, "docs": "http://localhost:8000/docs"}
                except:
                    pass

                try:
                    import socket
                    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                    sock.settimeout(2)
                    result = sock.connect_ex(('localhost', 3000))
                    sock.close()
                    results["BentoML"]["port_check"] = "âœ… ÐŸÐ¾Ñ€Ñ‚ 3000 Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚" if result == 0 else "âŒ ÐŸÐ¾Ñ€Ñ‚ 3000 Ð·Ð°ÐºÑ€Ñ‹Ñ‚"
                except:
                    results["BentoML"]["port_check"] = "âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð¿Ð¾Ñ€Ñ‚"

                return results

            gr.Markdown("### ðŸ“‹ ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹ batch Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²:")
            gr.Markdown("""
            ```bash
            # FastAPI batch
            curl -X POST http://localhost:8000/predict/batch \\
              -H "Content-Type: application/json" \\
              -d '{"texts": ["Ð¢ÐµÐºÑÑ‚ 1", "Ð¢ÐµÐºÑÑ‚ 2", "Ð¢ÐµÐºÑÑ‚ 3"]}'

            # BentoML batch
            curl -X POST http://localhost:3000/predict_batch \\
              -H "Content-Type: application/json" \\
              -d '{"texts": ["Ð¢ÐµÐºÑÑ‚ 1", "Ð¢ÐµÐºÑÑ‚ 2", "Ð¢ÐµÐºÑÑ‚ 3"]}'

            # BentoML single (text/plain)
            curl -X POST http://localhost:3000/predict \\
              -H "Content-Type: text/plain" \\
              -d "Ð¢ÐµÐºÑÑ‚ Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ"
            ```
            """)

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)
